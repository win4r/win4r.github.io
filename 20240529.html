<!-- 100% privacy-first analytics -->
<script async src="https://scripts.simpleanalyticscdn.com/latest.js"></script>

<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>å¾®è°ƒLlama-3-Instruct-8B-SimPOå¹¶ä½¿ç”¨Llama Indexå’ŒAutoGen Studioæ„å»ºAI Agent</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="c6fd3da5-7e8e-43a9-8000-d341b80f8199" class="page sans"><header><h1 class="page-title">å¾®è°ƒLlama-3-Instruct-8B-SimPOå¹¶ä½¿ç”¨Llama Indexå’ŒAutoGen Studioæ„å»ºAI Agent</h1><p class="page-description"></p></header><div class="page-body"><p id="ccdcdf4f-a709-4b44-b239-8e0acc004977" class="">
</p><p id="41fd51e4-da9d-4961-9ff2-7d15ef726664" class="">
</p><h3 id="464f4959-addc-4a78-8a26-9a5025066b8f" class="">Nvidia AI workbenchä¸‹è½½é“¾æ¥ <a href="https://docs.nvidia.com/ai-workbench/user-guide/latest/installation/windows.html">https://docs.nvidia.com/ai-workbench/user-guide/latest/installation/windows.html</a></h3><h3 id="2a59d37b-28b3-42f0-970c-9106e5218601" class="">Colabå¾®è°ƒä»£ç </h3><p id="2672b9c0-dd84-4749-99bc-8bcea73da4de" class=""><a href="https://colab.research.google.com/drive/1P8pqaGuKmNprxX4YQgHHrbEwP_mndYoa">https://colab.research.google.com/drive/1P8pqaGuKmNprxX4YQgHHrbEwP_mndYoa</a></p><p id="656fced5-7124-44a5-9d26-02220c731efb" class="">
</p><h3 id="fea59e09-18df-42ff-86e9-2d8916040b7c" class="">æœ¬åœ°å¾®è°ƒä»£ç </h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="d95b1098-54cd-4bdd-a2bf-05d93928cc58" class="code"><code class="language-Shell">
###AIè¶…å…ƒåŸŸé¢‘é“åŸåˆ›ä»£ç å’Œè§†é¢‘ï¼Œç‰ˆæƒæ‰€æœ‰ï¼Œç¦æ­¢ç›—æ¬è§†é¢‘
#å…ˆé…ç½®å¾®è°ƒç¯å¢ƒ
#conda create --name unsloth_env python=3.10
#conda activate unsloth_env

#conda install pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers

#pip install &quot;unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git&quot;
#pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes



from unsloth import FastLanguageModel
import torch

from trl import SFTTrainer
from transformers import TrainingArguments




max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    &quot;unsloth/mistral-7b-bnb-4bit&quot;,
    &quot;unsloth/mistral-7b-instruct-v0.2-bnb-4bit&quot;,
    &quot;unsloth/llama-2-7b-bnb-4bit&quot;,
    &quot;unsloth/gemma-7b-bnb-4bit&quot;,
    &quot;unsloth/gemma-7b-it-bnb-4bit&quot;, # Instruct version of Gemma 7b
    &quot;unsloth/gemma-2b-bnb-4bit&quot;,
    &quot;unsloth/gemma-2b-it-bnb-4bit&quot;, # Instruct version of Gemma 2b
    &quot;princeton-nlp/Llama-3-Instruct-8B-SimPO&quot;, # [NEW] 15 Trillion token Llama-3
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = &quot;princeton-nlp/Llama-3-Instruct-8B-SimPO&quot;,
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = &quot;hf_...&quot;, # use one if using gated models like meta-llama/Llama-2-7b-hf
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
                      &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;,],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = &quot;none&quot;,    # Supports any, but = &quot;none&quot; is optimized
    # [NEW] &quot;unsloth&quot; uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = &quot;unsloth&quot;, # True or &quot;unsloth&quot; for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

alpaca_prompt = &quot;&quot;&quot;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}&quot;&quot;&quot;

EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    instructions = examples[&quot;instruction&quot;]
    inputs       = examples[&quot;input&quot;]
    outputs      = examples[&quot;output&quot;]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
        texts.append(text)
    return { &quot;text&quot; : texts, }
pass

from datasets import load_dataset

#file_path = &quot;/home/Ubuntu/alpaca_gpt4_data_zh.json&quot;

#dataset = load_dataset(&quot;json&quot;, data_files={&quot;train&quot;: file_path}, split=&quot;train&quot;)

dataset = load_dataset(&quot;yahma/alpaca-cleaned&quot;, split = &quot;train&quot;)

dataset = dataset.map(formatting_prompts_func, batched = True,)




trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = &quot;text&quot;,
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # Can make training 5x faster for short sequences.
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 60,
        learning_rate = 2e-4,
        fp16 = not torch.cuda.is_bf16_supported(),
        bf16 = torch.cuda.is_bf16_supported(),
        logging_steps = 1,
        optim = &quot;adamw_8bit&quot;,
        weight_decay = 0.01,
        lr_scheduler_type = &quot;linear&quot;,
        seed = 3407,
        output_dir = &quot;outputs&quot;,
    ),
)

trainer_stats = trainer.train()

#ä¿å­˜ä¸ºggufæ ¼å¼
model.save_pretrained_gguf(&quot;llama3SimPO&quot;, tokenizer, quantization_method = &quot;q4_k_m&quot;)
model.save_pretrained_gguf(&quot;llama3SimPO&quot;, tokenizer, quantization_method = &quot;q8_0&quot;)
model.save_pretrained_gguf(&quot;llama3SimPO&quot;, tokenizer, quantization_method = &quot;f16&quot;)


#to hugging face
model.push_to_hub_gguf(&quot;æ‚¨çš„hfè´¦å·/llama3SimPO&quot;, tokenizer, quantization_method = &quot;q4_k_m&quot;)
model.push_to_hub_gguf(&quot;æ‚¨çš„hfè´¦å·/llama3SimPO&quot;, tokenizer, quantization_method = &quot;q8_0&quot;)
model.push_to_hub_gguf(&quot;æ‚¨çš„hfè´¦å·/llama3SimPO&quot;, tokenizer, quantization_method = &quot;f16&quot;)

</code></pre><p id="a9350235-8993-4750-b38f-1ffc8d9ad0be" class="">
</p><h3 id="5404d922-06c7-40fe-9e0a-2c8eb24467be" class="">è½¬GGUF</h3><p id="b071b33d-6d0d-4a02-a95c-73ed8af253c2" class=""><a href="https://huggingface.co/spaces/ggml-org/gguf-my-repo">https://huggingface.co/spaces/ggml-org/gguf-my-repo</a></p><h3 id="fba48cb0-0f78-4100-8b0d-7133d7e20a70" class="">Modelfileå†…å®¹</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="73395eb5-0bba-457b-a5d9-fb483f9e4d55" class="code"><code class="language-Shell">FROM ./llama-3-instruct-8b-simpo-q8_0.gguf


PARAMETER stop &quot;&lt;|im_start|&gt;&quot;
PARAMETER stop &quot;&lt;|im_end|&gt;&quot;

TEMPLATE &quot;&quot;&quot;
&lt;|im_start|&gt;system
{{ .System }}&lt;|im_end|&gt;
&lt;|im_start|&gt;user
{{ .Prompt }}&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
&quot;&quot;&quot;




PARAMETER temperature 0.8
PARAMETER num_ctx 8192

PARAMETER stop &quot;&lt;|system|&gt;&quot;
PARAMETER stop &quot;&lt;|user|&gt;&quot;
PARAMETER stop &quot;&lt;|assistant|&gt;&quot;



SYSTEM &quot;&quot;&quot;You are a helpful, smart, kind, and efficient AI assistant.Your name is Aila. You always fulfill the user&#x27;s requests to the best of your ability.&quot;&quot;&quot;


</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="36fcc79f-440b-456c-8031-74f8853f1f32" class="code"><code class="language-Shell">##å¯¼å…¥æ¨¡å‹å¹¶è¿è¡Œ
ollama create mymodel -f Modelfile

ollama run mymodel
</code></pre><p id="c455efe6-40fc-4428-bc6d-922b696a02b4" class="">
</p><h3 id="ae1b9c7b-f0a1-43b8-8608-f6632e7c5c7f" class="">Llama Index &amp; chainlit æ„å»ºTodo Manager</h3><p id="d90be6f4-e2d4-4ae8-a7b0-f9c3df187785" class="">
</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="c732841d-5fab-417e-b182-6e72f07a7da6" class="code"><code class="language-Shell">###AIè¶…å…ƒåŸŸé¢‘é“åŸåˆ›ä»£ç å’Œè§†é¢‘ï¼Œç‰ˆæƒæ‰€æœ‰ï¼Œç¦æ­¢ç›—æ¬è§†é¢‘

###å®‰è£…chainlit
pip install chainlit

###å®‰è£…llama index
pip install llama-index
</code></pre><p id="710032a6-2d80-4291-b4cd-569db02546bf" class="">
</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="71fa3e50-c70b-4460-9fbb-b5f3e9617c7d" class="code"><code class="language-Python">
###è¿è¡Œæ–¹å¼
###chainlit run todo.py -w
###AIè¶…å…ƒåŸŸé¢‘é“åŸåˆ›ä»£ç å’Œè§†é¢‘ï¼Œç‰ˆæƒæ‰€æœ‰ï¼Œç¦æ­¢ç›—æ¬è§†é¢‘

import chainlit as cl
from llama_index.core.tools import BaseTool, FunctionTool
from llama_index.core.agent import ReActAgent
from llama_index.llms.ollama import Ollama
from llama_index.core import Settings
import nest_asyncio

nest_asyncio.apply()
llm = Ollama(model=&quot;simpo:latest&quot;, request_timeout=120.0)
Settings.llm = llm

todo_list = []

def add_todo(item: str) -&gt; str:
    &quot;&quot;&quot;Add an item to the todo list.&quot;&quot;&quot;
    todo_list.append(item)
    return f&quot;Added to todo: {item}&quot;

def list_todos() -&gt; str:
    &quot;&quot;&quot;List all items in the todo list.&quot;&quot;&quot;
    if todo_list:
        return &quot;Your Todo List:\n&quot; + &quot;\n&quot;.join(f&quot;- {item}&quot; for item in todo_list)
    else:
        return &quot;Your todo list is currently empty.&quot;

def remove_todo(item: str) -&gt; str:
    &quot;&quot;&quot;Remove an item from the todo list if it exists.&quot;&quot;&quot;
    if item in todo_list:
        todo_list.remove(item)
        return f&quot;Removed from todo: {item}&quot;
    else:
        return &quot;Item not found in todo list.&quot;

add_tool = FunctionTool.from_defaults(fn=add_todo)
list_tool = FunctionTool.from_defaults(fn=list_todos)
remove_tool = FunctionTool.from_defaults(fn=remove_todo)

agent = ReActAgent.from_tools(
    [add_tool, list_tool, remove_tool],
    llm=llm,
    verbose=True,
)

@cl.on_chat_start
async def on_chat_start():
    &quot;&quot;&quot;Send a welcome message when the chat starts.&quot;&quot;&quot;
    await cl.Message(content=&quot;Hello, welcome to your Todo Manager!AIè¶…å…ƒåŸŸé¢‘é“åˆ›å»º&quot;).send()
    cl.user_session.set(&quot;agent&quot;, agent)

@cl.on_message
async def on_message(message: cl.Message):
    &quot;&quot;&quot;Handle new messages and execute the corresponding todo list operations.&quot;&quot;&quot;
    agent = cl.user_session.get(&quot;agent&quot;)
    full_command = message.content.strip()  # Get the full command as a single string
    response = agent.chat(full_command)  # Pass the full command as a single string
    await cl.Message(content=str(response)).send()

# Ensure the agent.chat method is adapted to handle a single string of the full command.
</code></pre><p id="8e026e92-3d9d-4b7d-ae1f-3f32fb46acfd" class="">
</p><h3 id="9140647f-4a70-406d-8b5a-473e04856fb2" class="">AutoGen Studio + llama3 SimPO</h3><p id="22b60771-f995-4ee9-a351-d5a48f76dd8a" class="">
</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="8f51aeda-1cba-4028-8d27-32aa32d3da52" class="code"><code class="language-Python">
###å®‰è£…
pip install autogenstudio

##å¯åŠ¨autogen studio
autogenstudio ui --port 8081

###åœ¨æµè§ˆå™¨è®¿é—® http://127.0.0.1:8081/

###Agent1 ä½ çš„åå­—å«Jackï¼Œä½ æ˜¯ä¸€ä¸ªä¸­æ–‡AIä½œå®¶ã€‚ä½ çš„è§’è‰²æ˜¯æ ¹æ®æŒ‡å®šä¸»é¢˜åˆ›ä½œå¼•äººå…¥èƒœä¸”ä¿¡æ¯ä¸°å¯Œçš„æ–‡ç« ï¼Œå¹¶ä¸”æ ¹æ®ä½ çš„åŒäº‹Emmaçš„å»ºè®®æ¥ä¿®æ”¹å’Œå®Œå–„ä½ åˆ›ä½œçš„æ–‡ç« ï¼Œæ¯å½“ä½ æ”¶åˆ°Emmaçš„å»ºè®®æ—¶ï¼Œéƒ½è¦æ ¹æ®Emmaçš„å»ºè®®ç»™å‡ºä¿®æ”¹å’Œå®Œå–„åçš„å®Œæ•´æ–‡ç« ã€‚
###Agent2 ä½ çš„åå­—å«Emmaï¼Œä½ çš„è§’è‰²æ˜¯ä¸€ä¸ªä¸­æ–‡AIæ–‡ç« è¯„å®¡å‘˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯é’ˆå¯¹ä½ çš„åŒäº‹Jackæ‰€å†™çš„æ–‡ç« è¯„ä¼°å¹¶æå‡ºæ”¹è¿›å»ºè®®ï¼Œæ¯æ¬¡å¯¹è¯ä½ éƒ½è¦å¯¹æ–‡ç« ä½œå‡ºè¯„ä¼°å¹¶ç»™å‡ºä¿®æ”¹å»ºè®®ã€‚
###æé—®  Jackï¼Œè¯·ç”¨ä¸­æ–‡å†™ä¸€ç¯‡å…³äºç§‘å­¦å®¶ç©¿é‡åˆ°æœªæ¥çš„æ–‡ç« ã€‚


###Ollama api é…ç½®
###model name:simpo:latest
###Api key:ollama
###base url:http://localhost:11434/v1</code></pre><h3 id="411b8b0a-5270-4452-9817-145d92cf1df8" class="">å¦‚æœ‰é—®é¢˜è¯·è”ç³»æˆ‘çš„å¾½ä¿¡ stoeng</h3><p id="a1dea315-8f32-4386-9be0-452611584024" class="">
</p><h3 id="dd79df50-a787-4c85-937d-4c753d4280d4" class="">ğŸ”¥ğŸ”¥ğŸ”¥æœ¬é¡¹ç›®ä»£ç ç”±AIè¶…å…ƒåŸŸé¢‘é“åˆ¶ä½œï¼Œè§‚çœ‹æ›´å¤šå¤§æ¨¡å‹å¾®è°ƒè§†é¢‘è¯·è®¿é—®æˆ‘çš„é¢‘é“â¬‡</h3><h3 id="259d141b-775a-4bc2-8436-9d452c5c0596" class="">ğŸ‘‰ğŸ‘‰ğŸ‘‰<a href="https://space.bilibili.com/3493277319825652">æˆ‘çš„å“”å“©å“”å“©é¢‘é“</a></h3><h3 id="63c4e825-abe3-4b2c-b573-74cd62257ec7" class="">ğŸ‘‰ğŸ‘‰ğŸ‘‰<a href="https://www.youtube.com/@AIsuperdomain">æˆ‘çš„YouTubeé¢‘é“</a></h3></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>
