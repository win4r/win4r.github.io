<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>pixtral 12B轻松实现视频智能分析</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 237, 214, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 237, 214, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(0, 0, 0, 0.06); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(249, 228, 188, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="68c42c31-310f-4e17-a1ef-78b1dfcc9386" class="page sans"><header><h1 class="page-title">pixtral 12B轻松实现视频智能分析</h1><p class="page-description"></p></header><div class="page-body"><p id="6db50afd-772d-4589-8171-b82213f00a46" class="">
</p><blockquote id="752b4756-4b17-4b6c-822c-432df25823a4" class="">Pixtral 12B是由法国初创公司Mistral AI最新推出的多模态人工智能模型。这个模型意义重大，因为它代表了Mistral首次尝试结合文本和图像处理能力，使其有望与OpenAI和Anthropic等公司的领先人工智能模型展开竞争。<h2 id="48d070ee-b614-4a51-829b-5210739b9f8b" class="">Pixtral 12B的主要特点</h2><p id="7472fea2-b694-41ea-8af6-1658dc092d84" class=""><strong>模型架构和参数</strong></p><ul id="f7fcebef-7225-4caf-990d-38b1f0968e7d" class="bulleted-list"><li style="list-style-type:disc"><strong>基础模型</strong>：基于之前发布的文本模型Nemo 12B。</li></ul><ul id="03f86c47-62a5-4392-aaf8-bc5ecf7bb3ae" class="bulleted-list"><li style="list-style-type:disc"><strong>总参数量</strong>：跨40层共120亿参数。</li></ul><ul id="7fa3ea3b-a14f-4d5b-87e3-37faa278cea5" class="bulleted-list"><li style="list-style-type:disc"><strong>视觉适配器</strong>：集成了4亿参数的视觉适配器，增强了处理视觉数据的能力。</li></ul><ul id="5bf1786d-189d-4879-a121-fa06b686621d" class="bulleted-list"><li style="list-style-type:disc"><strong>隐藏维度</strong>：具有14,336个隐藏维度和32个注意力头，实现了广泛的计算处理。</li></ul><ul id="e17018df-84da-4ef8-85aa-d5075d7ae8b9" class="bulleted-list"><li style="list-style-type:disc"><strong>图像分辨率</strong>：能够处理1024 x 1024像素分辨率的图像，将其分割成16 x 16像素的小块。</li></ul><ul id="59d73fd8-2746-4243-b9a2-1caf0f1bc2b6" class="bulleted-list"><li style="list-style-type:disc"><strong>词元词汇量</strong>：扩展了词汇量至131,072个词元，包括三个新的专用于图像处理的特殊词元：<code>img</code>、<code>img_break</code>和<code>img_end</code>。</li></ul><ul id="0e2c02a6-663e-4cd3-ac5d-fa2cd05ecefb" class="bulleted-list"><li style="list-style-type:disc"><strong>位置编码</strong>：采用2D RoPE（旋转位置嵌入）来增强对图像空间关系的理解。</li></ul><p id="7abb9baf-e10d-4936-a013-cc5adc7d6077" class=""><strong>功能</strong><br/>Pixtral 12B允许用户通过URL或base64编码输入图像，能够执行多种任务，如：<br/></p><ul id="684b1fa3-4d61-4a55-bf29-5870993e0af7" class="bulleted-list"><li style="list-style-type:disc"><strong>图像描述</strong>：为上传的图像生成描述性文字。</li></ul><ul id="379456d2-3055-41a1-8f42-0739294d1ce5" class="bulleted-list"><li style="list-style-type:disc"><strong>物体识别</strong>：识别并计数图像中的物体。</li></ul><ul id="4f6ed9d3-e157-49ec-acb2-6814977884ca" class="bulleted-list"><li style="list-style-type:disc"><strong>视觉问答</strong>：根据图像内容回答问题。</li></ul><p id="0033556f-30d3-42bf-8714-f325e00fd7dc" class="">该模型的架构支持同时处理文本和图像，使其在内容分析和数据解释方面的应用具有多样性。</p><h2 id="f0e1b7b8-6744-4de6-bdef-c6a398ccbe5a" class="">可访问性和许可</h2><p id="f8e458a1-e2c5-4e54-adf5-8d65498cdfd7" class="">Pixtral 12B可通过GitHub和Hugging Face等多个平台下载，也可以通过磁力链接获取。它以Apache 2.0许可证发布，允许用户自由使用、修改和商业化该模型。然而，用于训练模型的具体数据集尚未公开，这引发了对训练数据可能涉及的版权问题的质疑。此外，其完整的许可条款尚未完全明确，预计未来将有更多相关信息发布。</p><h2 id="f9d1e601-d63b-4b7f-bd7e-df44216d2746" class="">未来发展</h2><p id="8c8655db-259b-415d-9f79-296bd9956416" class="">Mistral计划将Pixtral 12B整合到其聊天机器人Le Chat和API平台La Platforme中，使开发者和用户更容易使用。随着人工智能社区开始试验Pixtral 12B，预计将会对其能力和性能有更深入的了解。</p><p id="ff870800-54c9-4252-a8f0-057df21b250f" class="">Pixtral 12B标志着Mistral AI在多模态人工智能领域迈出了重要一步，有望增强生成式人工智能应用的能力。这个模型通过结合先进的文本处理能力和新增的视觉处理功能，为用户提供了一个强大而灵活的多模态AI工具。</p></blockquote><p id="d41cdd4b-af63-4d1b-a365-9c5f2514fe79" class="">
</p><h3 id="de24c838-dbbf-4b81-b404-8443e32c4141" class="">环境</h3><ul id="3df182ec-cea0-4f0f-9a28-df394cf020f2" class="bulleted-list"><li style="list-style-type:disc">Ubuntu</li></ul><ul id="8b53e9e5-c20b-4173-a38c-4cd1154e9ac1" class="bulleted-list"><li style="list-style-type:disc">Nvidia RTX A6000*2</li></ul><ul id="4924ff7a-f2e4-4e97-9878-30ca13092f01" class="bulleted-list"><li style="list-style-type:disc">vLLM</li></ul><p id="f0ec2df8-b3c0-4838-97ee-214416bf9608" class="">
</p><h2 id="786a4f90-3303-4505-8cfe-39ffadcdba1d" class=""><strong>👉👉👉如有问题请联系我的徽信 stoeng</strong></h2><h2 id="7fdf9f8b-a62b-43de-8904-abc66d29df75" class=""><strong>🔥🔥🔥本项目代码由AI超元域频道制作，观看更多大模型微调视频请访问我的频道⬇</strong></h2><h3 id="1756acc1-a99d-4867-9363-3a7354857847" class=""><strong>👉👉👉</strong><strong><a href="https://space.bilibili.com/3493277319825652">我的哔哩哔哩频道</a></strong></h3><h3 id="8b11423e-1dea-4ba8-b1c5-0d11d3d51132" class=""><strong>👉👉👉</strong><strong><a href="https://www.youtube.com/@AIsuperdomain">我的YouTube频道</a></strong></h3><h3 id="dcfaeeb7-848e-4cde-9783-275fb0aa0702" class=""><strong>👉👉👉我的开源项目 </strong><strong><a href="https://github.com/win4r/AISuperDomain">https://github.com/win4r/AISuperDomain</a></strong></h3><p id="94fb01ec-ed18-4de2-a4c9-f0516bcd2bb8" class="">
</p><h3 id="e7d12978-eb9e-4985-80a6-d6c5f9a61357" class="">安装</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="cf543916-cf95-4b31-b80c-d2cb81d221c1" class="code"><code class="language-Shell">pip install --upgrade mistral_common vllm


vllm serve mistralai/Pixtral-12B-2409 --tokenizer_mode mistral --limit_mm_per_prompt &#x27;image=4&#x27; --max_num_batched_tokens 16384 --gpu-memory-utilization 0.95 --max-model-len 65536

vllm serve mistralai/Pixtral-12B-2409 --tokenizer_mode mistral --limit_mm_per_prompt &#x27;image=4&#x27; --max_num_batched_tokens 65536 --gpu-memory-utilization 0.95 --max-model-len 65536

pip install -U &quot;huggingface_hub[cli]&quot;

huggingface-cli login



curl http://64.247.196.11:8000/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d &#x27;{
    &quot;model&quot;: &quot;mistralai/Pixtral-12B-2409&quot;,
    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello, how are you?&quot;}]
  }&#x27;




curl --location &#x27;http://64.247.196.11:8000/v1/chat/completions&#x27; \
--header &#x27;Content-Type: application/json&#x27; \
--header &#x27;Authorization: Bearer token&#x27; \
--data &#x27;{
    &quot;model&quot;: &quot;mistralai/Pixtral-12B-2409&quot;,
    &quot;messages&quot;: [
      {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot; : &quot;text&quot;, &quot;text&quot;: &quot;Describe this image in detail please in Chinese.&quot;},
            {&quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: {&quot;url&quot;: &quot;https://s3.amazonaws.com/cms.ipressroom.com/338/files/201808/5b894ee1a138352221103195_A680%7Ejogging-edit/A680%7Ejogging-edit_hero.jpg&quot;}},
            {&quot;type&quot; : &quot;text&quot;, &quot;text&quot;: &quot;and this one as well. Answer in Chinese.&quot;},
            {&quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: {&quot;url&quot;: &quot;https://www.wolframcloud.com/obj/resourcesystem/images/a0e/a0ee3983-46c6-4c92-b85d-059044639928/6af8cfb971db031b.png&quot;}}
        ]
      }
    ]
  }&#x27;



from openai import OpenAI

# 正确初始化 OpenAI 客户端
client = OpenAI(
    base_url=&quot;http://64.247.196.11:8000/v1&quot;,
    api_key=&quot;test&quot;
)

response = client.chat.completions.create(
  model=&quot;mistralai/Pixtral-12B-2409&quot;,
  messages=[
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: [
        {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What&#x27;s in this image?&quot;},
        {
          &quot;type&quot;: &quot;image_url&quot;,
          &quot;image_url&quot;: {
            &quot;url&quot;: &quot;https://s3.amazonaws.com/cms.ipressroom.com/338/files/201808/5b894ee1a138352221103195_A680%7Ejogging-edit/A680%7Ejogging-edit_hero.jpg&quot;,
          },
        },
      ],
    }
  ],
  max_tokens=1024,
)

print(response.choices[0])




import base64
from openai import OpenAI

def encode_image(image_path):
    with open(image_path, &quot;rb&quot;) as image_file:
        return base64.b64encode(image_file.read()).decode(&#x27;utf-8&#x27;)

# 初始化 OpenAI 客户端
client = OpenAI(
    base_url=&quot;http://64.247.196.11:8000/v1&quot;,
    api_key=&quot;test&quot;
)

# 本地图片路径
image_path = &quot;./dog.jpg&quot;

# 编码图片
base64_image = encode_image(image_path)

response = client.chat.completions.create(
  model=&quot;mistralai/Pixtral-12B-2409&quot;,
  messages=[
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: [
        {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What&#x27;s in this image?&quot;},
        {
          &quot;type&quot;: &quot;image_url&quot;,
          &quot;image_url&quot;: {
            &quot;url&quot;: f&quot;data:image/jpeg;base64,{base64_image}&quot;,
          },
        },
      ],
    }
  ],
  max_tokens=1024,
)

print(response.choices[0])




</code></pre><p id="28490018-9852-43e2-8d5b-7bd1059c061d" class="">
</p><h3 id="d4288381-f2d3-4e08-b649-92c88a240678" class="">stream</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="0f3aeda7-86b9-471c-b70d-e7c1ade05a2a" class="code"><code class="language-Shell">from openai import OpenAI
import sys

# 初始化 OpenAI 客户端
client = OpenAI(
    base_url=&quot;http://64.247.196.11:8000/v1&quot;,
    api_key=&quot;test&quot;
)

# 创建流式 completion 请求
stream = client.chat.completions.create(
    model=&quot;mistralai/Pixtral-12B-2409&quot;,
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What&#x27;s in this image?&quot;},
                {
                    &quot;type&quot;: &quot;image_url&quot;,
                    &quot;image_url&quot;: {
                        &quot;url&quot;: &quot;https://s3.amazonaws.com/cms.ipressroom.com/338/files/201808/5b894ee1a138352221103195_A680%7Ejogging-edit/A680%7Ejogging-edit_hero.jpg&quot;,
                    },
                },
            ],
        }
    ],
    max_tokens=1024,
    stream=True  # 启用流式输出
)

# 处理流式响应
for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end=&#x27;&#x27;, flush=True)

print()  # 最后打印一个换行</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="0aa06a3d-0be5-4688-b49e-25e41dce8468" class="code"><code class="language-Shell">import base64
import sys
from openai import OpenAI


def encode_image(image_path):
    with open(image_path, &quot;rb&quot;) as image_file:
        return base64.b64encode(image_file.read()).decode(&#x27;utf-8&#x27;)


# 初始化 OpenAI 客户端
client = OpenAI(
    base_url=&quot;http://64.247.196.11:8000/v1&quot;,
    api_key=&quot;test&quot;
)

# 本地图片路径
image_path = &quot;./dog.jpg&quot;

# 编码图片
base64_image = encode_image(image_path)

# 创建流式 completion 请求
stream = client.chat.completions.create(
    model=&quot;mistralai/Pixtral-12B-2409&quot;,
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What&#x27;s in this image?&quot;},
                {
                    &quot;type&quot;: &quot;image_url&quot;,
                    &quot;image_url&quot;: {
                        &quot;url&quot;: f&quot;data:image/jpeg;base64,{base64_image}&quot;,
                    },
                },
            ],
        }
    ],
    max_tokens=1024,
    stream=True  # 启用流式输出
)

# 处理流式响应
for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end=&#x27;&#x27;, flush=True)

print()  # 最后打印一个换行</code></pre><h3 id="08ce2403-9f75-4b3e-9684-dc3ce92e1474" class="">UI Chatbot</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="b70d1959-6967-4678-abd9-90fd7c3899eb" class="code"><code class="language-Shell">#pip install streamlit
#streamlit run bot.py


import streamlit as st
import base64
from openai import OpenAI
from io import BytesIO


def encode_image(image_file):
    return base64.b64encode(image_file.getvalue()).decode(&#x27;utf-8&#x27;)


def get_chat_response(client, messages):
    stream = client.chat.completions.create(
        model=&quot;mistralai/Pixtral-12B-2409&quot;,
        messages=messages,
        max_tokens=1024,
        stream=True
    )
    return stream


st.title(&quot;多模态Chatbot&quot;)

# 初始化OpenAI客户端
client = OpenAI(
    base_url=&quot;http://64.247.196.11:8000/v1&quot;,
    api_key=&quot;test&quot;
)

# 初始化会话状态
if &quot;messages&quot; not in st.session_state:
    st.session_state.messages = []

# 显示聊天历史
for message in st.session_state.messages:
    with st.chat_message(message[&quot;role&quot;]):
        st.markdown(message[&quot;content&quot;])

# 图片上传
uploaded_file = st.file_uploader(&quot;上传图片&quot;, type=[&quot;jpg&quot;, &quot;jpeg&quot;, &quot;png&quot;])

# 用户输入
user_input = st.chat_input(&quot;输入你的问题&quot;)

if uploaded_file and user_input:
    # 编码图片
    base64_image = encode_image(uploaded_file)

    # 创建新的用户消息
    new_message = {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: user_input},
            {
                &quot;type&quot;: &quot;image_url&quot;,
                &quot;image_url&quot;: {
                    &quot;url&quot;: f&quot;data:image/jpeg;base64,{base64_image}&quot;,
                },
            },
        ]
    }

    st.session_state.messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input})

    with st.chat_message(&quot;user&quot;):
        st.markdown(user_input)
        st.image(uploaded_file, caption=&quot;上传的图片&quot;, use_column_width=True)

    # 获取AI响应
    with st.chat_message(&quot;assistant&quot;):
        message_placeholder = st.empty()
        full_response = &quot;&quot;
        for chunk in get_chat_response(client, [new_message]):
            if chunk.choices[0].delta.content is not None:
                full_response += chunk.choices[0].delta.content
                message_placeholder.markdown(full_response + &quot;▌&quot;)
        message_placeholder.markdown(full_response)

    st.session_state.messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: full_response})

st.sidebar.title(&quot;使用说明&quot;)
st.sidebar.markdown(&quot;&quot;&quot;
1. 上传一张图片
2. 在输入框中输入你的问题
3. 等待AI分析图片并回答你的问题
&quot;&quot;&quot;)</code></pre><h3 id="4ffab6bf-eb35-4561-a0ca-6a9b3be755c2" class="">chainlit</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="262fdfa3-e2e3-466a-9fa0-7c67d6b0ca43" class="code"><code class="language-Shell">import base64
import chainlit as cl
from openai import OpenAI

# 初始化 OpenAI 客户端
client = OpenAI(
    base_url=&quot;http://64.247.196.48:8000/v1&quot;,
    api_key=&quot;test&quot;
)

def encode_image(image_path):
    with open(image_path, &quot;rb&quot;) as image_file:
        return base64.b64encode(image_file.read()).decode(&#x27;utf-8&#x27;)

@cl.on_chat_start
async def start():
    await cl.Message(&quot;欢迎使用图像分析应用!请选择一张图片并输入您的问题，然后点击发送。&quot;).send()

@cl.on_message
async def main(message: cl.Message):
    if not message.elements:
        await cl.Message(&quot;请上传一张图片并输入您的问题。&quot;).send()
        return

    image = message.elements[0]
    if not image.mime.startswith(&quot;image&quot;):
        await cl.Message(&quot;请上传一个有效的图片文件。&quot;).send()
        return

    question = message.content
    if not question:
        await cl.Message(&quot;请输入关于图片的问题。&quot;).send()
        return

    await process_image(image.path, question)

async def process_image(image_path, question):
    base64_image = encode_image(image_path)

    stream = client.chat.completions.create(
        model=&quot;mistralai/Pixtral-12B-2409&quot;,
        messages=[
            {
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: [
                    {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: question},
                    {
                        &quot;type&quot;: &quot;image_url&quot;,
                        &quot;image_url&quot;: {
                            &quot;url&quot;: f&quot;data:image/jpeg;base64,{base64_image}&quot;,
                        },
                    },
                ],
            }
        ],
        max_tokens=1024,
        stream=True
    )

    msg = cl.Message(content=&quot;&quot;)
    await msg.send()

    full_response = &quot;&quot;
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            content = chunk.choices[0].delta.content
            full_response += content
            await msg.stream_token(content)

    await msg.update()

if __name__ == &quot;__main__&quot;:
    cl.run()</code></pre><p id="e2538925-a5bb-4777-971d-7304f3cb56f7" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>